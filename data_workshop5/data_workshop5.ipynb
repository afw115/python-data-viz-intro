{
 "metadata": {
  "name": "",
  "signature": "sha256:76ec60c6f989b44999d9809cd77293d32c645eb1d1ed52c933d6567c35143332"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "An Introduction to Plotting and Data Analysis in Python 5"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clyde Fare and Jo\u00e3o Pedro Malhado, Imperial College London (contact: [python@imperial.ac.uk](python@imperial.ac.uk))\n",
      "\n",
      "Exercise 1 uses material adapted from work by Paul Wilde. Exercise 2 is addapted and uses the dataset from the article <a href=\"http://dx.doi.org/10.2307/2682899\">F. J. Anscombe, The American Statistician, 27(1):17-21 (1973)</a>. This notebook is licensed under a [Creative Commons Attribution 4.0 (CC-by) license](http://creativecommons.org/licenses/by/4.0/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's load the pylab and non-linear fitting machinery and get that out of the way."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline\n",
      "import scipy.optimize"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this exercise we will be studying the kinetics of the decomposition reaction of $N_2 O_5$\n",
      "\n",
      "$$2 N_2 O_{5 (g)} \\longrightarrow 4NO_{2 (g)} + O_{2 (g)}$$\n",
      "\n",
      "The rate of a chemical reaction is how fast the concentration of one of the reactants or products changes during the chemical reaction. In the case of the reaction under study the rate of the reaction is proportional to the concentration of $N_2 O_5$, it is said to obey a first order rate law:\n",
      "\n",
      "$$rate=- \\frac{d[N_2 O_5]}{d t} =k [N_2 O_5] ,$$\n",
      "\n",
      "where $k$ is the *rate constant*. This is a differential equation and its solution gives us the concentration of $N_2 O_5$ as a function of time.\n",
      "\n",
      "$$[N_2 O_5]=[N_2 O_5]_0 e^{-k t}\ufffc$$\n",
      "\n",
      "where $[N_2 O_5]_0$ is the initial concentration and $t$ is the time. (If you want to know where the solution comes from click [here](https://wiki.ch.ic.ac.uk/wiki/index.php?title=Solving_the_Rate_Equation).)\n",
      "\n",
      "We are interested in determining the value of the rate constant $k$. In order to do this, the concentration of $N_2 O_5$ was recorded as a function of time at $65^{\\circ}C$, and stored in file [N2O5vst_65C.dat](files/N2O5vst_65C.dat), where the first column is time in seconds and the second column is concentration in $\\text{mol.dm}^{-3}$.\n",
      "\n",
      "* Load the data and plot the concentration of $N_2 O_5$ against time, label the axes and include the units in the label."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Create an appropriate python function for the variation of $[N_2 O_5]$ with time that through non linear fitting will allow us to extract the rate constant\n",
      "\n",
      "* Fit the function to the data and report the value of the rate constant and the uncertainty associated with it. \n",
      "\n",
      "* Superimpose the fit with the experimental data to check your fit is meaningful."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some other colleagues followed the same proceedure as above but at different temperatures. The following table shows values of the rate constant and its uncertainty for different temperatures.\n",
      "\n",
      "<table>\n",
      "<tr><th>$T/^{\\circ} C$</th><th>$k/s^{-1}$</th><th>$\\sigma_k/s^{-1}$</th><tr>\n",
      "<tr><td>20</td><td>0.0000188</td><td>0.0000089</td></tr>\n",
      "<tr><td>30</td><td>0.0000694</td><td>0.0000073</td></tr>\n",
      "<tr><td>40</td><td>0.000260</td><td>0.000023</td></tr>\n",
      "<tr><td>50</td><td>0.000888</td><td>0.000067</td></tr>\n",
      "<tr><td>60</td><td>0.00293</td><td>0.00021</td></tr>\n",
      "</table>\n",
      "\n",
      "The variation of the rate constant with temperature can give important information about the chemical reaction. The rate constant of an elementary reaction varies in general with temperature according to the Arrhenius equation that you will cover in more detail in your kinetics lectures\n",
      "\n",
      "$$k(T)=A e^{-\\frac{E_a}{R T}} ,$$\n",
      "\n",
      "where $E_a$ is activation energy, $A$ is the so called pre-exponential factor (which is related to the frequency of molecular collisions) and $R$ is the ideal gas constant.\n",
      "\n",
      "* Create an appropriate function that through non linear fitting will allow us to extract the pre-exponential factor and the activation energy.\n",
      "\n",
      "* From the rate constant values on the table above, and the rate constant value you have determined, fit the data to the determine the pre-exponential factor and the activation energy for this reaction. \n",
      "\n",
      "* Plot values predicted from your fitted function alongside the experimental values to visualise your result.\n",
      "\n",
      "*Note 1*: The uncertainties associated with each rate constant are different, you should take this into account. \n",
      "\n",
      "*Note 2*: Units of temperature must be consistent with units of R."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you are having difficulties finding a suitable initial guess for the non-linear Arrhenius fit, or if you managed to extract your activation energy quickly try a different approach:\n",
      "\n",
      "The Arrhenius equation is a non-linear function of $T$, but can be converted into a linear form by taking the natural logarithm of both sides of the equation:\n",
      "\n",
      "$$\\ln(k)=\\ln(A)-\\frac{E_a}{R}\\frac{1}{T} .$$\n",
      "\n",
      "We obtain a linear relationship between $\\ln(k)$ and $\\frac{1}{T}$, where the intercept is $\\ln(A)$ and the slope $-\\frac{E_a}{R}$. It should be noted that it is only possible in a few cases to transform a non-linear equation into a linear one in this way.\n",
      "\n",
      "* Plot $\\ln(k)$ against $\\frac{1}{T}$\n",
      "* Use polyfit to extract the gradient and intercept of the $\\ln(k)$ vs. $\\frac{1}{T}$ data (ignore any uncertainties associated with the data when performing this fit) \n",
      "* Use these to determine the activation energy and the pre-exponential factor. \n",
      "* How do your results compare with the non-linear fit above?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the previous question we performed a fit without taking the uncertainties assoicated with the data into account. This is equivalent to considering all the $\\ln(k)$ data points have the same uncertainty. From the $T$ vs $k$ table above we know this is not the case, and we should take the uncertainties into account. Note however that $\\sigma_k$ is not the same as $\\sigma_{\\ln(k)}$, and also $\\sigma_{\\ln(k)}\\neq\\ln(\\sigma_k)$! There are [general methods](http://en.wikipedia.org/wiki/Propagation_of_uncertainty#Simplification) to propagate the uncertainty of a value upon a mathematical operation. For the present purposes however, we will focus on the case of the logarithm at hand:\n",
      "\n",
      "$$\\sigma_{\\ln(k)}=\\frac{\\sigma_k}{k} .$$\n",
      "\n",
      "* Plot the values of $\\ln(k)$ as a function of $\\frac{1}{T}$ with appropriate error bars. Note the values of the error bars at low $T$ values. What do you expect the relative contribution of these data points to the linear fit to be?\n",
      "\n",
      "* Fit the data taking into account the error bars (recall the differences between functions *polyfit* and *curve_fit* in dealing with data point uncertainty). How do the results obtained via this proceedure compare with the ones obtained via direct exponential fit?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Exercise 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The dataset in the CSV file <a href=\"anscombe.csv\">anscombe.csv</a> is formed by 4 sets of x and y values in the form\n",
      "\n",
      "<table>\n",
      "<tr><td colspan=2>Set 1</td><td colspan=2>Set 2</td><td colspan=2>Set 3</td><td colspan=2>Set 4</td></tr>\n",
      "<tr><td>x</td><td>y</td><td>x</td><td>y</td><td>x</td><td>y</td><td>x</td><td>y</td></tr>\n",
      "</table>\n",
      "\n",
      "* Load the data onto the notebook\n",
      "* Calculate the mean of x and y for all sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Do a linear fit of y against x for each set and compare the values of the parameters obtained."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Based on your analysis so far, could you comclude that the four sets of data correspond to the same experiment and agree with each other?\n",
      "\n",
      "* Plot your data along with your linear fit in each case."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Would you now reach the same conclusion to the question above?\n",
      "\n",
      "*Conclusion: You should always plot your data! Do not blindly rely on statistical measures.*\n",
      "\n",
      "For which of the datasets is the linear fit meaninful? Plotting the residuals helps you answering this question."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}